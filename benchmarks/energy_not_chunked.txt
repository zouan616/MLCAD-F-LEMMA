[SPLASH] Benchmarks to run: fft

[SPLASH] [========== Running benchmark fft ==========]
[SPLASH] Setting up run directory: /tmp/tmpwAhFr5
[SPLASH] Running 'SNIPER_APP_LD_PRELOAD=$LD_PRELOAD; unset LD_PRELOAD; /home/adith/simulator_sniper/final_sniper_00/run-sniper -n 4 -m 'localhost' -d '/home/adith/simulator_sniper/final_sniper_00/benchmarks' -s categorical_without_pacman:8000 --roi --curdir=/home/adith/simulator_sniper/final_sniper_00/benchmarks  --  /home/adith/simulator_sniper/final_sniper_00/benchmarks/splash2/splash2/codes/kernels/fft/FFT -m10 -p4':
[SPLASH] [---------- Beginning of output ----------]
[SNIPER] Start
Executing Python script /home/adith/simulator_sniper/final_sniper_00/benchmarks/sim.scripts.py
Creating an actor-critic model for core number: 0
Loading previously trained model
Creating an actor-critic model for core number: 1
Loading previously trained model
Creating an actor-critic model for core number: 2
Loading previously trained model
Creating an actor-critic model for core number: 3
Loading previously trained model
EnergyStats Interval: 1000
Q-Learning Interval   : 8000 2
Global Budget Interval: 120000 30
[SNIPER] --------------------------------------------------------------------------------
[SNIPER] Sniper using SIFT/trace-driven frontend
[SNIPER] Running pre-ROI region in  CACHE_ONLY mode
[SNIPER] Running application ROI in DETAILED mode
[SNIPER] Running post-ROI region in FAST_FORWARD mode
[SNIPER] --------------------------------------------------------------------------------

FFT with Blocking Transpose
   1024 Complex Doubles
   4 Processors
   65536 Cache lines
   16 Byte line size
   4096 Bytes per page

[HOOKS] Entering ROI
[SNIPER] Enabling performance models
[SNIPER] Setting instrumentation mode to DETAILED
Global Available Budget, Residual Budget 95.0 88.1386229033
Current Core, IPC 0 0.293771885943
Current Core, IPC 1 0.0
Current Core, IPC 2 0.0
Current Core, IPC 3 0.0
Budgets for each core: [5.42935854, 4.8582187730999999, 4.8582187730999999, 4.8582187730999999]
current_power state0:  6.0326206
curr_ipc state0:  0.293771885943
State.0  tensor([-0.9943,  1.2419,  1.3407, -0.0955, -0.2451, -0.5199, -0.3772, -0.6084,
        -0.1590, -0.4583, -0.0541, -0.1590, -0.1588,  0.3547,  0.3623, -0.1588,
        -0.1553,  0.3623,  2.1117])
current_power reward0:  6.0326206
curr_ipc reward0:  0.293771885943
CURRENT ENERGY IN REWARD FUNCTION.0: 48226
Reward.0 -2.72253841406 True
current_power state1:  5.398020859
curr_ipc state1:  0.0
State.1  tensor([-1.2278, -0.7826,  1.3407, -0.2468, -0.2722, -0.8028, -0.3772, -0.8257,
        -0.3243, -0.7843, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward1:  5.398020859
curr_ipc reward1:  0.0
CURRENT ENERGY IN REWARD FUNCTION.1: 43176
Reward.1 -2.6990104295 True
current_power state2:  5.398020859
curr_ipc state2:  0.0
State.2  tensor([-1.2278, -0.7826,  1.3407, -0.2468, -0.2722, -0.8028, -0.3772, -0.8257,
        -0.3243, -0.7843, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward2:  5.398020859
curr_ipc reward2:  0.0
CURRENT ENERGY IN REWARD FUNCTION.2: 43176
Reward.2 -2.6990104295 True
current_power state3:  5.398020859
curr_ipc state3:  0.0
State.3  tensor([-1.2278, -0.7826,  1.3407, -0.2468, -0.2722, -0.8028, -0.3772, -0.8257,
        -0.3243, -0.7843, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward3:  5.398020859
curr_ipc reward3:  0.0
CURRENT ENERGY IN REWARD FUNCTION.3: 43176
Reward.3 -2.6990104295 True
Total Elapsed Time 5.15698020964e+11
Total Elapsed Time 0.0
current_power state0:  2.9018536
curr_ipc state0:  0.521886609595
State.0  tensor([-0.8130,  1.6135, -0.4111, -0.8419, -0.2064, -0.3865,  0.0365, -0.4405,
         0.0665, -0.3487, -0.1986,  0.0665,  0.0667,  0.1096,  0.1170,  0.0667,
         0.1555,  0.1170,  1.2932])
current_power reward0:  2.9018536
curr_ipc reward0:  0.521886609595
CURRENT ENERGY IN REWARD FUNCTION.0: 69925
Reward.0 -12.1156380904 False
current_power state1:  2.8180928
curr_ipc state1:  0.00101799593214
State.1  tensor([-1.2270,  3.3664, -0.4111, -0.8619, -0.1793, -0.6972,  0.0365, -0.7298,
         0.0214, -0.6725,  0.2782,  0.0214,  0.0216,  0.8881,  0.8961,  0.0216,
         0.0519,  0.8961, -0.1164])
current_power reward1:  2.8180928
curr_ipc reward1:  0.00101799593214
CURRENT ENERGY IN REWARD FUNCTION.1: 60788
Reward.1 -10.1996118696 False
current_power state2:  5.5426763
curr_ipc state2:  0.000116476921569
State.2  tensor([-1.2277,  1.9370,  1.3407, -0.2123, -0.2296, -0.7792,  0.0365, -0.7893,
        -0.0688, -0.7692, -0.2420, -0.0688, -0.0686, -0.0201, -0.0128, -0.0686,
        -0.0724, -0.0128, -0.7075])
current_power reward2:  5.5426763
curr_ipc reward2:  0.000116476921569
CURRENT ENERGY IN REWARD FUNCTION.2: 86497
Reward.2 -3.42217115758 True
current_power state3:  5.3981081
curr_ipc state3:  0.0
State.3  tensor([-1.2278, -0.7826,  1.3407, -0.2467, -0.2722, -0.8028,  0.0365, -0.8257,
        -0.3243, -0.7843, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward3:  5.3981081
curr_ipc reward3:  0.0
CURRENT ENERGY IN REWARD FUNCTION.3: 86352
Reward.3 -2.6994466345 True
current_power state0:  5.48888007
curr_ipc state0:  0.374131944444
State.0  tensor([-0.9305,  1.0649,  1.3407, -0.2251, -0.2722, -0.5389, -0.6765, -0.6238,
        -0.2642, -0.4797, -0.2709, -0.2642, -0.2640, -0.0489, -0.0561, -0.2640,
        -0.3418, -0.0561,  0.7476])
current_power reward0:  5.48888007
curr_ipc reward0:  0.374131944444
CURRENT ENERGY IN REWARD FUNCTION.0: 120514
Reward.0 0.0765242944444 True
current_power state1:  2.19496114
curr_ipc state1:  0.0309222094222
State.1  tensor([-1.2032,  1.9281, -0.4111, -1.0104, -0.2722, -0.7748, -0.6765, -0.7947,
        -0.2942, -0.7583, -0.3575, -0.2942, -0.2941, -0.1066, -0.0993, -0.2941,
        -0.3418, -0.0993, -0.7075])
current_power reward1:  2.19496114
curr_ipc reward1:  0.0309222094222
CURRENT ENERGY IN REWARD FUNCTION.1: 78904
Reward.1 -13.2853659561 False
current_power state2:  2.105028845
curr_ipc state2:  0.00949552498131
State.2  tensor([-1.2202, -0.7826, -0.4111, -1.0319, -0.2722, -0.7901, -0.6765, -0.8118,
        -0.3093, -0.7723, -0.4587, -0.3093, -0.3091, -0.3805, -0.3734, -0.3091,
        -0.3625, -0.3734, -0.7075])
current_power reward2:  2.105028845
curr_ipc reward2:  0.00949552498131
CURRENT ENERGY IN REWARD FUNCTION.2: 105088
Reward.2 -13.7564541155 False
current_power state3:  5.398020859
curr_ipc state3:  0.010660618089
State.3  tensor([-1.2193, -0.7826,  1.3407, -0.2468, -0.2722, -0.7901, -0.6765, -0.8118,
        -0.3093, -0.7723, -0.4442, -0.3093, -0.3091, -0.3661, -0.3590, -0.3091,
        -0.3625, -0.3590, -0.7075])
current_power reward3:  5.398020859
curr_ipc reward3:  0.010660618089
CURRENT ENERGY IN REWARD FUNCTION.3: 133028
Reward.3 -2.68834981141 True
current_power state0:  1.34786988
curr_ipc state0:  0.329894953345
State.0  tensor([-0.9656,  1.1980, -1.2870, -1.2124, -0.2529, -0.5913,  0.7847, -0.6393,
        -0.1439, -0.5566, -0.1986, -0.1439, -0.1438, -0.0201, -0.1282, -0.1438,
        -0.2175, -0.1282, -0.0709])
current_power reward0:  1.34786988
curr_ipc reward0:  0.329894953345
CURRENT ENERGY IN REWARD FUNCTION.0: 131950
Reward.0 -20.0775483467 False
current_power state1:  1.45748077
curr_ipc state1:  0.590389092522
State.1  tensor([-0.7586,  1.7415, -1.2870, -1.1863, -0.2413, -0.4825,  0.7847, -0.4112,
         0.4423, -0.5285,  0.1915,  0.4423,  0.4424,  1.1187,  1.1269,  0.4424,
         0.6529,  1.1269, -0.2528])
current_power reward1:  1.45748077
curr_ipc reward1:  0.590389092522
CURRENT ENERGY IN REWARD FUNCTION.1: 90907
Reward.1 -16.413300923 False
current_power state2:  2.4410651
curr_ipc state2:  0.435393734124
State.2  tensor([-0.8818,  1.6263, -0.4111, -0.9517, -0.2490, -0.4906,  0.7847, -0.4181,
         0.4423, -0.5373,  0.1626,  0.4423,  0.4424,  1.1475,  1.1557,  0.4424,
         0.6321,  1.1557, -0.5256])
current_power reward2:  2.4410651
curr_ipc reward2:  0.435393734124
CURRENT ENERGY IN REWARD FUNCTION.2: 124950
Reward.2 -11.6503746314 False
current_power state3:  2.5112607
curr_ipc state3:  0.426279863481
State.3  tensor([-0.8890,  1.6082, -0.4111, -0.9350, -0.2490, -0.5000,  0.7847, -0.4274,
         0.3671, -0.5467,  0.0326,  0.3671,  0.3673,  1.0322,  1.0403,  0.3673,
         0.5493,  1.0403,  0.5202])
current_power reward3:  2.5112607
curr_ipc reward3:  0.426279863481
CURRENT ENERGY IN REWARD FUNCTION.3: 153840
Reward.3 -11.308510502 False
current_power state0:  5.7801223
curr_ipc state0:  0.0550458724912
State.0  tensor([-1.1840,  1.0965,  1.3407, -0.1557, -0.2606, -0.7474, -0.2980, -0.7630,
        -0.2040, -0.7338, -0.4153, -0.2040, -0.2039, -0.2652, -0.2580, -0.2039,
        -0.2175, -0.2580, -0.4347])
current_power reward0:  5.7801223
curr_ipc reward0:  0.0550458724912
CURRENT ENERGY IN REWARD FUNCTION.0: 176211
Reward.0 -1.69877292751 True
current_power state1:  5.52649973
curr_ipc state1:  0.0273086330779
State.1  tensor([-1.2061,  0.5884,  1.3407, -0.2161, -0.2722, -0.7679, -0.2980, -0.7901,
        -0.2792, -0.7499, -0.4442, -0.2792, -0.2790, -0.4381, -0.4312, -0.2790,
        -0.3833, -0.4312, -0.7075])
current_power reward1:  5.52649973
curr_ipc reward1:  0.0273086330779
CURRENT ENERGY IN REWARD FUNCTION.1: 134975
Reward.1 -3.31409615142 True
current_power state2:  2.105338845
curr_ipc state2:  0.0060760763929
State.2  tensor([-1.2229,  4.4839, -0.4111, -1.0318, -0.2722, -0.7947, -0.2980, -0.8179,
        -0.3243, -0.7759, -0.4876, -0.3243, -0.3241, -0.4958, -0.4889, -0.3241,
        -0.3833, -0.4889, -0.7075])
current_power reward2:  2.105338845
curr_ipc reward2:  0.0060760763929
CURRENT ENERGY IN REWARD FUNCTION.2: 142047
Reward.2 -13.7583235641 False
current_power state3:  4.4602787
curr_ipc state3:  0.169487562083
State.3  tensor([-1.0931,  0.0687,  0.4648, -0.4703, -0.2490, -0.6019, -0.2980, -0.5790,
         0.1717, -0.6148, -0.0108,  0.1717,  0.1719,  0.2394,  0.2469,  0.1719,
         0.2799,  0.2469,  0.7476])
current_power reward3:  4.4602787
curr_ipc reward3:  0.169487562083
CURRENT ENERGY IN REWARD FUNCTION.3: 183052
Reward.3 -1.82021280342 False
current_power state0:  4.739451481
curr_ipc state0:  1.63450109478
State.0  tensor([ 0.0711,  0.6672, -0.4111, -0.4038, -0.2684, -0.0416, -0.9670, -0.0755,
        -0.3243, -0.0186, -0.2998, -0.3243, -0.3241, -0.2796, -0.5033, -0.3241,
        -0.3833, -0.5033, -0.7075])
current_power reward0:  4.739451481
curr_ipc reward0:  1.63450109478
CURRENT ENERGY IN REWARD FUNCTION.0: 209058
Reward.0 -1.81503420022 False
current_power state1:  4.739387227
curr_ipc state1:  1.44437509752
State.1  tensor([-0.0800,  0.6511, -0.4111, -0.4038, -0.2684, -0.1232, -0.9670, -0.1598,
        -0.3243, -0.0981, -0.2853, -0.3243, -0.3241, -0.2075, -0.2148, -0.3241,
        -0.3833, -0.2148, -0.7075])
current_power reward1:  4.739387227
curr_ipc reward1:  1.44437509752
CURRENT ENERGY IN REWARD FUNCTION.1: 165266
Reward.1 0.850217367019 False
current_power state2:  2.99279917
curr_ipc state2:  1.45305099696
State.2  tensor([-0.0731,  0.9191, -1.2870, -0.8202, -0.2684, -0.2681, -0.9670, -0.2936,
        -0.3243, -0.2499, -0.3864, -0.3243, -0.3241, -0.3084, -0.3302, -0.3241,
        -0.3833, -0.3302, -0.7075])
current_power reward2:  2.99279917
curr_ipc reward2:  1.45305099696
CURRENT ENERGY IN REWARD FUNCTION.2: 159638
Reward.2 -7.87404701854 False
current_power state3:  8.16673547
curr_ipc state3:  1.47410229343
State.3  tensor([-0.0564,  0.6239,  0.4648,  0.4134, -0.2684, -0.0251, -0.9670, -0.0601,
        -0.3243, -0.0014, -0.2853, -0.3243, -0.3241, -0.2652, -0.2725, -0.3241,
        -0.3833, -0.2725, -0.6166])
current_power reward3:  8.16673547
curr_ipc reward3:  1.47410229343
CURRENT ENERGY IN REWARD FUNCTION.3: 232720
Reward.3 -15.0684811911 True
current_power state0:  7.879648214
curr_ipc state0:  1.77628284836
State.0  tensor([ 0.1838,  0.4711,  0.4648,  0.3449, -0.2722,  0.1317, -0.9846,  0.0954,
        -0.3243,  0.1555, -0.2998, -0.3243, -0.3241, -0.3084, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward0:  7.879648214
curr_ipc reward0:  1.77628284836
CURRENT ENERGY IN REWARD FUNCTION.0: 270719
Reward.0 -10.4751655216 True
current_power state1:  4.475742458
curr_ipc state1:  1.5453125
State.1  tensor([ 2.1797e-04,  6.0403e-01, -4.1111e-01, -4.6664e-01, -2.6836e-01,
        -1.0919e-01, -9.8459e-01, -1.4203e-01, -3.2428e-01, -8.6684e-02,
        -2.9975e-01, -3.2428e-01, -3.2412e-01, -3.0840e-01, -3.0132e-01,
        -3.2412e-01, -3.8325e-01, -3.0132e-01, -7.0750e-01])
current_power reward1:  4.475742458
curr_ipc reward1:  1.5453125
CURRENT ENERGY IN REWARD FUNCTION.1: 200934
Reward.1 -0.3670690755 False
current_power state2:  4.889217794
curr_ipc state2:  1.55984375
State.2  tensor([ 0.0118,  0.5579, -0.4111, -0.3681, -0.2684, -0.0917, -0.9846, -0.1250,
        -0.3243, -0.0690, -0.2998, -0.3243, -0.3241, -0.3084, -0.3013, -0.3241,
        -0.3833, -0.3013, -0.7075])
current_power reward2:  4.889217794
curr_ipc reward2:  1.55984375
CURRENT ENERGY IN REWARD FUNCTION.2: 195475
Reward.2 1.4048486455 True
current_power state3:  11.43434485
curr_ipc state3:  1.58
State.3  tensor([ 0.0278,  0.4861,  1.3407,  1.1924, -0.2722,  0.0999, -0.9846,  0.0645,
        -0.3243,  0.1233, -0.2998, -0.3243, -0.3241, -0.3084, -0.3013, -0.3241,
        -0.3833, -0.3013, -0.7075])
current_power reward3:  11.43434485
curr_ipc reward3:  1.58
CURRENT ENERGY IN REWARD FUNCTION.3: 320407
Reward.3 -31.3006303845 True
current_power state0:  2.4048596
curr_ipc state0:  0.240222167329
State.0  tensor([-1.0369, -0.4565, -1.2870, -0.9604, -0.2490, -0.5146, -0.9318, -0.4808,
         0.4122, -0.5353,  0.1915,  0.4122,  0.4124,  0.2250,  0.2325,  0.4124,
         0.6114,  0.2325, -0.6166])
current_power reward0:  2.4048596
curr_ipc reward0:  0.240222167329
CURRENT ENERGY IN REWARD FUNCTION.0: 283046
Reward.0 -14.8822725327 False
current_power state1:  2.4181593
curr_ipc state1:  0.0623844038094
State.1  tensor([-1.1782,  0.0188, -0.4111, -0.9572, -0.2684, -0.7477, -0.9318, -0.7615,
        -0.2040, -0.7354, -0.4009, -0.2040, -0.2039, -0.3805, -0.3879, -0.2039,
        -0.2175, -0.3879, -0.7075])
current_power reward1:  2.4181593
curr_ipc reward1:  0.0623844038094
CURRENT ENERGY IN REWARD FUNCTION.1: 229701
Reward.1 -12.1379129617 False
current_power state2:  4.46830514
curr_ipc state2:  0.89421875
State.2  tensor([-0.5172,  0.2996, -0.4111, -0.4684, -0.2490, -0.2507, -0.9318, -0.2039,
         0.6527, -0.2811,  0.5816,  0.6527,  0.4124,  0.6430,  0.4056,  0.4124,
        -0.3833,  0.4056, -0.5256])
current_power reward2:  4.46830514
curr_ipc reward2:  0.89421875
CURRENT ENERGY IN REWARD FUNCTION.2: 230887
Reward.2 -1.0553494155 False
current_power state3:  1.337800701
curr_ipc state3:  0.0198333061383
State.3  tensor([-1.2120,  3.0531, -1.2870, -1.2148, -0.2722, -0.7785, -0.9318, -0.8025,
        -0.3243, -0.7593, -0.4442, -0.3243, -0.3241, -0.4237, -0.4167, -0.3241,
        -0.3833, -0.4167, -0.7075])
current_power reward3:  1.337800701
curr_ipc reward3:  0.0198333061383
CURRENT ENERGY IN REWARD FUNCTION.3: 331596
Reward.3 -17.5822570544 False
current_power state0:  11.009311859
curr_ipc state0:  1.658875
State.0  tensor([ 0.0905,  0.7341,  1.3407,  1.0911, -0.2722,  0.0902, -0.9846,  0.0869,
        -0.3243,  0.0921, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward0:  11.009311859
curr_ipc reward0:  1.658875
CURRENT ENERGY IN REWARD FUNCTION.0: 368290
Reward.0 -26.240891595 True
current_power state1:  4.850072845
curr_ipc state1:  1.63986887293
State.1  tensor([ 0.0754,  0.7018, -0.4111, -0.3774, -0.2722, -0.0684, -0.9846, -0.0763,
        -0.3243, -0.0628, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward1:  4.850072845
curr_ipc reward1:  1.63986887293
CURRENT ENERGY IN REWARD FUNCTION.1: 263123
Reward.1 1.59913923243 False
current_power state2:  3.882521545
curr_ipc state2:  1.57516747157
State.2  tensor([ 0.0239,  1.0441, -0.4111, -0.6081, -0.2722, -0.1930, -0.9846, -0.1938,
        -0.3243, -0.1917, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward2:  3.882521545
curr_ipc reward2:  1.57516747157
CURRENT ENERGY IN REWARD FUNCTION.2: 265204
Reward.2 -3.30331866893 False
current_power state3:  6.386165213
curr_ipc state3:  1.60069604514
State.3  tensor([ 0.0442,  0.8967,  0.4648, -0.0112, -0.2722, -0.0811, -0.9846, -0.0848,
        -0.3243, -0.0784, -0.5309, -0.3243, -0.3241, -0.5390, -0.5321, -0.3241,
        -0.3833, -0.5321, -0.7075])
current_power reward3:  6.386165213
curr_ipc reward3:  1.60069604514
CURRENT ENERGY IN REWARD FUNCTION.3: 384236
Reward.3 -6.03903615436 True
current_power state0:  2.185558
curr_ipc state0:  0.238732732895
State.0  tensor([-1.0381, -0.0480, -1.2870, -1.0127, -0.2722, -0.6109, -0.9582, -0.5991,
         0.1717, -0.6164, -0.0397,  0.1717,  0.1568, -0.0201, -0.0128,  0.1568,
        -0.3833, -0.0128, -0.6620])
current_power reward0:  2.185558
curr_ipc reward0:  0.238732732895
CURRENT ENERGY IN REWARD FUNCTION.0: 386400
Reward.0 -15.9802699671 False
current_power state1:  6.3056202
curr_ipc state1:  0.972084062396
State.1  tensor([-0.4553,  0.4341,  0.4648, -0.0304, -0.2722, -0.1597, -0.9582, -0.1235,
         0.4723, -0.1834,  0.2637,  0.4723,  0.4725,  0.2970,  0.2757,  0.4725,
        -0.3833,  0.2757, -0.7075])
current_power reward1:  6.3056202
curr_ipc reward1:  0.972084062396
CURRENT ENERGY IN REWARD FUNCTION.1: 322187
Reward.1 -6.2649230721 True
current_power state2:  2.680102
curr_ipc state2:  0.817885639065
State.2  tensor([-0.5778, -0.2290, -1.2870, -0.8948, -0.2722, -0.2419, -0.9582, -0.2047,
         0.5174, -0.2660,  0.3071,  0.5174,  0.3973,  0.3403,  0.2902,  0.3973,
        -0.3833,  0.2902, -0.7075])
current_power reward2:  2.680102
curr_ipc reward2:  0.817885639065
CURRENT ENERGY IN REWARD FUNCTION.2: 286355
Reward.2 -10.0726982264 False
current_power state3:  4.0834453
curr_ipc state3:  0.96765625
State.3  tensor([-0.4588,  0.4139, -0.4111, -0.5602, -0.2722, -0.2441, -0.9582, -0.2054,
         0.4573, -0.2691,  0.2637,  0.4573,  0.4574,  0.3115,  0.3046,  0.4574,
        -0.3833,  0.3046, -0.6166])
current_power reward3:  4.0834453
curr_ipc reward3:  0.96765625
CURRENT ENERGY IN REWARD FUNCTION.3: 421754
Reward.3 -2.9062111155 False
current_power state0:  5.77472612
curr_ipc state0:  0.0454402811774
State.0  tensor([-1.1917,  0.7450,  1.3407, -0.1569, -0.2722, -0.7524, -0.9494, -0.7746,
        -0.2642, -0.7344, -0.3287, -0.2642, -0.2640, -0.2075, -0.2003, -0.2640,
        -0.3833, -0.2003, -0.7075])
current_power reward0:  5.77472612
curr_ipc reward0:  0.0454402811774
CURRENT ENERGY IN REWARD FUNCTION.0: 432656
Reward.0 -1.68139761882 True
current_power state1:  2.269062069
curr_ipc state1:  0.0685617682049
State.1  tensor([-1.1733,  0.7324, -0.4111, -0.9928, -0.2722, -0.7421, -0.9494, -0.7630,
        -0.2642, -0.7250, -0.3864, -0.2642, -0.2640, -0.2507, -0.2580, -0.2640,
        -0.3833, -0.2580, -0.5256])
current_power reward1:  2.269062069
curr_ipc reward1:  0.0685617682049
CURRENT ENERGY IN REWARD FUNCTION.1: 340083
Reward.1 -12.8772217523 False
current_power state2:  1.42231665
curr_ipc state2:  0.0359696402505
State.2  tensor([-1.1992,  1.9222, -1.2870, -1.1946, -0.2722, -0.7530, -0.9494, -0.7754,
        -0.2792, -0.7349, -0.4009, -0.2792, -0.2790, -0.3372, -0.3302, -0.2790,
        -0.3833, -0.3302, -0.7075])
current_power reward2:  1.42231665
curr_ipc reward2:  0.0359696402505
CURRENT ENERGY IN REWARD FUNCTION.2: 296880
Reward.2 -17.1435409752 False
current_power state3:  5.7470649
curr_ipc state3:  0.0331204623267
State.3  tensor([-1.2014,  2.7850,  1.3407, -0.1635, -0.2722, -0.7564, -0.9494, -0.7793,
        -0.2642, -0.7380, -0.4587, -0.2642, -0.2640, -0.4093, -0.4023, -0.2640,
        -0.3833, -0.4023, -0.7075])
current_power reward3:  5.7470649
curr_ipc reward3:  0.0331204623267
CURRENT ENERGY IN REWARD FUNCTION.3: 467020
Reward.3 -4.41111017217 True
[TRACE:2] -- DONE --
[TRACE:1] -- DONE --
[TRACE:3] -- DONE --
[SNIPER] Disabling performance models
[SNIPER] Leaving ROI after 42.30 seconds
[SNIPER] Simulated 0.4M instructions, 1.0M cycles, 0.39 IPC
[SNIPER] Simulation speed 8.8 KIPS (2.2 KIPS / target core - 452310.3ns/instr)
[SNIPER] Sampling: executed 10.18% of simulated time in detailed mode
[SNIPER] Setting instrumentation mode to FAST_FORWARD
[TRACE:0] -- DONE --
[HOOKS] Leaving ROI

                 PROCESS STATISTICS
            Computation      Transpose     Transpose
 Proc          Time            Time        Fraction
    0                56              5       0.08929

                 TIMING INFORMATION
Start time                        :      -1844408245
Initialization finish time        :      -1844407780
Overall finish time               :      -1844407724
Total time with initialization    :              521
Total time without initialization :               56
Overall transpose time            :                5
Overall transpose fraction        :          0.08929

[RECORD-TRACE] Using the Pin frontend (sift/recorder)
Sim Hook End.. Displaying Final Powers and Energies of each core.
0.STATIC POWER (W) 1.7155273
0.DYNAMIC POWER (W) 0.389587181
0.STATIC ENERGY (nJ) 273871
0.DYNAMIC ENERGY (nJ) 170329
1.STATIC POWER (W) 2.9283802
1.DYNAMIC POWER (W) 0.87702971
1.STATIC ENERGY (nJ) 218352
1.DYNAMIC ENERGY (nJ) 144048
2.STATIC POWER (W) 2.9283802
2.DYNAMIC POWER (W) 1.43058384
2.STATIC ENERGY (nJ) 196456
2.DYNAMIC ENERGY (nJ) 123469
3.STATIC POWER (W) 4.7896069
3.DYNAMIC POWER (W) 0.97106422
3.STATIC ENERGY (nJ) 320841
3.DYNAMIC ENERGY (nJ) 180341
TOTAL ENERGY (nJ) 1627707
Total Elapsed Time 6.00004622084e+11
Total Elapsed Time 6.00004622084e+11
Discounted Rewards.0 tensor(-80.7455)
NUMBER OF REWARDS: 11
LOSS.0: tensor(0.4154, grad_fn=<AddBackward0>)
Discounted Rewards.1 tensor(-59.8363)
NUMBER OF REWARDS: 11
LOSS.1: tensor(0.6910, grad_fn=<AddBackward0>)
Discounted Rewards.2 tensor(-64.1616)
NUMBER OF REWARDS: 11
LOSS.2: tensor(0.2915, grad_fn=<AddBackward0>)
Discounted Rewards.3 tensor(-74.2940)
NUMBER OF REWARDS: 11
LOSS.3: tensor(0.4373, grad_fn=<AddBackward0>)
ALL LOSSES:  1.83515870571
ALL REWARDS: -279.037414551
[SNIPER] End
[SNIPER] Elapsed time: 45.43 seconds
[SPLASH] [----------    End of output    ----------]
[SPLASH] Done.
